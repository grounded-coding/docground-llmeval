from tqdm import tqdm

import requests
import json
import re
from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor

class PromptScorer:
    # This is a technical class. It receives a pre-built prompt and submits it to the local LLM API.
    def __init__(self, api_url, metric_config_file, num_retries=3):
        self.api_url = api_url
        self.num_retries = num_retries

        with open(metric_config_file, "r") as conf:
            self.metric_config = json.load(conf)

    def build_prompt(self, dimension, output, src=None, context=None):
        """
            Add prompt instructions for the framework
            
            dimension: specific dimension to be evaluated
            src: source input for different NLG tasks. For example, source document for summarization 
                and dialogue history for dialogue response generation.
            output: output text generated by the models
            ref: human-annotataed groundtruth
            context: the context needed to evaluate several specific dimension. For example,
                    additional factual information when evaluating engagingness and groundedness in dialogues.
        """
        # TODO Fix this with dimension definitions from the evaluator
        full_prompt = []
        for i in range(len(output)):
            if dimension == 'appropriateness':
                cur_input = 'question: Is this a natural response in the dialogue? </s> response: ' + output[i]
            elif dimension == 'accuracy':
                cur_input = 'question: Is this response consistent with knowledge in the fact? </s> response: '\
                            + output[i] + ' </s> fact: ' + context[i]
            else:
                raise NotImplementedError('The input format for this dimension is still undefined. Please customize it first.')
            full_prompt.append(cur_input)
        return full_prompt

    def build_and_submit_prompt(self, i, output_list, src_list, context_list, dimension, method="likert"):
        prompt = self.build_prompt(dimension, output_list[i], src_list[i], context_list[i])

        data = {
            "inputs": prompt,
            "parameters": self.metric_config["gen_params"]
        }

        headers = {
            'Content-Type': 'application/json'
        }

        success = False
        for _ in range(self.num_retries):
            success = False
            response_text = requests.post(self.api_url, json=data, headers=headers).text
            response_text = json.loads(response_text)["generated_text"]

            if method == "winrate":
                regex_str = rf'\nMore {dimension} response: ([1-2])'
            else:
                regex_str = rf'\n{dimension.capitalize()} Score: ([12345])'

            match = re.search(regex_str, response_text)
            if match:
                winner = match.group(1)
                explanation = response_text[:match.start()].rstrip("\n").lstrip("\n")
                success = True
                break
        if not success:
            winner, explanation = -1, "Error in syntax retrieval"

        return {dimension: winner, "id": i, "explanation": explanation}


    def score(self, output_list, src_list, context_list, dimension, batch_size=8):
        # Builds a prompt and submits it in distributed fashion to the local LLM API.
        # Then extracts score and explanation from the response.
        winexpls = []

        with ThreadPoolExecutor(max_workers=batch_size) as executor:
            futures = [executor.submit(self.build_and_submit_prompt, i, output_list, src_list, context_list, dimension)
                        for i, output in enumerate(output_list)]
            for future in tqdm(as_completed(futures), total=len(output_list)):
                winexpl = future.result()
                winexpls.extend()

        winexpls = sorted(winexpls, key=lambda x: x["id"])
        return winexpl